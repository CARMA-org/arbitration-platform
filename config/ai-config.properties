# AI Service Configuration for CARMA Arbitration Platform
# =======================================================
#
# This file configures API keys and settings for real LLM integration.
#
# SECURITY NOTE: Do NOT commit actual API keys to version control.
# Use environment variables or a local .env file instead.
#
# To use:
# 1. Copy this file to ai-config.local.properties (gitignored)
# 2. Replace placeholder values with your actual keys
# OR
# 3. Set environment variables: OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY

# ========================================================================
# PROVIDER SETTINGS
# ========================================================================

# Default provider (OPENAI, ANTHROPIC, GEMINI, LOCAL)
ai.default.provider=${AI_DEFAULT_PROVIDER:GEMINI}

# ========================================================================
# OPENAI CONFIGURATION
# ========================================================================
ai.openai.api.key=${OPENAI_API_KEY:}
ai.openai.api.url=https://api.openai.com/v1
ai.openai.model.chat=gpt-4
ai.openai.model.embedding=text-embedding-3-small
ai.openai.model.image=dall-e-3

# ========================================================================
# ANTHROPIC CONFIGURATION
# ========================================================================
ai.anthropic.api.key=${ANTHROPIC_API_KEY:}
ai.anthropic.api.url=https://api.anthropic.com/v1
ai.anthropic.model=claude-3-opus-20240229
ai.anthropic.version=2023-06-01

# ========================================================================
# GOOGLE GEMINI CONFIGURATION
# ========================================================================
ai.gemini.api.key=${GEMINI_API_KEY:}
ai.gemini.api.url=https://generativelanguage.googleapis.com/v1beta/models
ai.gemini.model=gemini-2.5-flash

# ========================================================================
# LOCAL/OLLAMA CONFIGURATION
# ========================================================================
ai.local.api.url=http://localhost:11434
ai.local.model=llama2

# ========================================================================
# GENERAL SETTINGS
# ========================================================================
ai.max.tokens=4096
ai.temperature=0.7
ai.timeout.seconds=60
ai.enabled=${AI_ENABLED:true}
ai.log.requests=true

# ========================================================================
# SAFETY LIMITS
# ========================================================================
ai.rate.limit.requests.per.minute=60
ai.rate.limit.tokens.per.minute=100000
